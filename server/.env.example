# LLM Provider Configuration
# Set to "openai" to use OpenAI API or "local" to use local Llama model
LLM_PROVIDER=openai

OPENAI_API_KEY=your_api_key_here

# Local Llama Configuration (required if LLM_PROVIDER=local)
# URL for the local Llama API (default: http://localhost:11434/api/generate)
LOCAL_LLM_URL=http://localhost:11434/api/generate
# Model name to use with local Llama (default: llama3)
LOCAL_LLM_MODEL=llama3

# Logging level (info, debug, trace, warn, error)
RUST_LOG=info